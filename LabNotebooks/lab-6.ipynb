{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9834037,"sourceType":"datasetVersion","datasetId":6031904}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Lab 6**\n\nTI3155TU Deep Learning (2024 - 2025)\n\nAuthor: Elena Congeduti \n\nMost of the material has been adapted from \n[Transfer learning with ResNet-50 in PyTorch](https://www.kaggle.com/code/pmigdal/transfer-learning-with-resnet-50-in-pytorch/notebook).","metadata":{}},{"cell_type":"markdown","source":"# Instructions\nWe recommend that you fork the lab notebooks by selecting `Copy & Edit` on the notebook's homepage. This will create a copy in your Kaggle repository, allowing you to work on it and save your progress as you go. Kaggle provides a pre-configured virtual environment, which means that most of the libraries we will use are already downloaded and ready to use. Therefore, you typically do **not** need to `pip install` additional resources.\n\nAlternatively, to work on Google Colab, you just need to select the `Open in Colab` option in the notebook's homepage menu. Finally, if you want to work locally, you will need to set up your own virtual environment. Check the Lab Instructions in [Learning Material](https://brightspace.tudelft.nl/d2l/le/content/682797/Home?itemIdentifier=D2L.LE.Content.ContentObject.ModuleCO-3812764) on Brightspace for detailed information on the virtual environment configuration.\n\nThese labs include programming exercises and insight questions. Follow the instructions in the notebook. Fill in the text blocks to answer the questions and write your own code to solve the programming tasks within the designated part of the code blocks:\n\n```python\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\n\n\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n```\n\n\nSolutions will be shared the week after the lab is published. Note that these labs are designed for practice and are therefore **ungraded**.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n!pip install torchsummary\nfrom torchsummary import summary\n\nimport torch\nfrom torchvision import datasets, models, transforms\n\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this lab we will work at the example we have discussed in class: classification of alien and predator images. Let us start by exploring the dataset and defining the learning problem.","metadata":{}},{"cell_type":"markdown","source":"# 1 Dataset \nWhen starting a deep learning project, it's always a good idea to take a close look at the data. The dataset is already uploaded in the notebook, and you can find it in the Input section on the right. There, you can have a first loot at the folder structure, the number of samples, format etc. This will make it easier to navigate the dataset and access individual samples.\n\nIf you take a moment to explore the folder and youâ€™ll find a training set with 347 images of aliens and 347 of predators and a validation set with 100 images of aliens and 100 images of predators.","metadata":{}},{"cell_type":"code","source":"#set random seed\nnp.random.seed(42)\n\n# input path\ninput_path = \"../input/images/data/\"\n\n#sample two random images from training and validation\ntrain_ind = np.random.randint(0,347,size = 2)\nvalid_ind = np.random.randint(0,100,size =2)\n\nsample_img_paths = []\n\nsample_img_paths.append('train/alien/'+str(train_ind[0])+'.jpg')\nsample_img_paths.append('train/predator/'+str(train_ind[1])+'.jpg')\nsample_img_paths.append('validation/alien/'+str(valid_ind[0])+'.jpg')\nsample_img_paths.append('validation/predator/'+str(valid_ind[1])+'.jpg')\n\n#Opens the images at a specific path and collect them in a list\nimg_list = [Image.open(input_path + img_path) for img_path in sample_img_paths]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Python library PIL (from the Pillow library) allows for handling images in various formats, such as JPEG, PNG and others. The function `PIL.Image` can open, process, and convert them into into image objects that can be manipulated, analyzed directly or converted to arrays. \n\nTo convert an image to a tensor, we use the `transforms` function from the `torchvision` library. This collection of functions is essential for preprocessing images and using them within the PyTorch framework.","metadata":{}},{"cell_type":"code","source":"#Example of one image \nimage = img_list[0]\n\n#Define the transformation \ntransform =  transforms.ToTensor()\n\n#Convert it to tensor\ntorch_image = transform(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's write a function that will plot the images together with given labels.","metadata":{}},{"cell_type":"code","source":"def plot_images(img_list, labels, columns=4):\n    \n    # Rows needed based on the number of images in the list\n    rows = len(img_list) // columns + (1 if len(img_list) % columns > 0 else 0)\n\n    # Create subplots\n    fig, axes = plt.subplots(rows, columns, figsize=(columns * 3, rows * 3))\n    axes = axes.flatten()  # Flatten the axes array for easy iteration\n    \n    # Loop through all images\n    for i in range(len(img_list)):\n        ax = axes[i]\n        ax.imshow(img_list[i])\n        ax.set_title(labels[i]) \n        ax.axis('off') \n\n    # Hide any extra subplots that aren't used\n    for j in range(len(img_list), len(axes)):\n        axes[j].axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_images(img_list, ['train - alien','train - predator','valid - alien','valid - predator']  ) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n**Task 1.1:** Use the transformation defined above to convert the images into tensors and then  check some relevant features for the sample of images. Which shape do they have? What is the range of values? \n****\n","metadata":{}},{"cell_type":"code","source":"#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\n    \n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 Preprocessing\n\nNow we know the dataset and we have defined the learning task: binary classification for images. As the dataset is quite small and the computation resources limited, in order to have a good classifier we will need to use transfer learning. We will use a pre-trained ResNet model over ImageNet.","metadata":{}},{"cell_type":"markdown","source":"****\n**Question 2.1**\nPreprocessing techniques should be consistent with the model choices. Do you think the current data format aligns with that of ImageNet? What additional preprocessing steps are necessary for preparing the images?","metadata":{}},{"cell_type":"markdown","source":"<font color=\"green\">  Write your answer here\n</font>\n\n****","metadata":{}},{"cell_type":"markdown","source":"The next step consists of defining the transformations for the preprocessing part. ","metadata":{}},{"cell_type":"code","source":"#Normalization\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n#Transformations\ndata_transforms = {\n    'train':\n    transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize\n    ]),\n    'validation':\n    transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize\n    ]),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the transformation for the training set, you can see the additional transformations\n```\ntransforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\ntransforms.RandomHorizontalFlip(),\n```\nThese transformations augment the training dataset by generating modified versions of the original training images by randomly applying affine transformations to the images as shearing, scaling, or flipping. \n\nWe will cover data augmentation in Lecture 7. For now, it's important to note that these transformations are **only** applied to the training set. This is crucial to ensure a valid assessment of the model's performance; the validation and test sets must remain reliable representations of the model's performance on unseen, real-world data.\n","metadata":{}},{"cell_type":"markdown","source":"Now we create the dataset using DataLoader which efficiently load and batch data, enabling easy iteration over large datasets during training and evaluation.","metadata":{}},{"cell_type":"code","source":"#Set the seed\ntorch.manual_seed(42)\n\nimage_datasets = {\n    'train': \n    datasets.ImageFolder(input_path + 'train', data_transforms['train']),\n    'validation': \n    datasets.ImageFolder(input_path + 'validation', data_transforms['validation'])\n}\n\ndataloaders = {\n    'train':\n    torch.utils.data.DataLoader(image_datasets['train'],\n                                batch_size=32,\n                                shuffle=True,\n                                num_workers=0),  \n    'validation':\n    torch.utils.data.DataLoader(image_datasets['validation'],\n                                batch_size=32,\n                                shuffle=False,\n                                num_workers=0)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3 Building and Training the Model\nNow we first import the pre-treined ResNet model, we build our own model according to the problem we want to solve and then we train it. Let's start by checking that we are actually running the session on a GPU.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If the output you see \"cpu\", you must check that in Settings --> Accelerator you have chosen one of the GPU options. Otherwise, you will wait a little while training your model...","metadata":{}},{"cell_type":"code","source":"#Load the pretrained resnet model from torchvision models\nmodel = models.resnet50(pretrained=True).to(device)\n\n#Check the\nprint (summary(model ,(3,224,224)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n**Question 3.1:** do you think there is anything we need to modify in the architecture of the network? Have a look at the last layer before answering.\n","metadata":{}},{"cell_type":"code","source":"#Check final layer \nprint(model.fc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color=\"green\">  Write your answer here\n</font>\n\n****","metadata":{}},{"cell_type":"markdown","source":"We now modify the network structure by replacing the last fully connected layer with two fully connected layers, whose parameters will learn the specific features of aliens and predators.","metadata":{}},{"cell_type":"code","source":"#Loop over all the parameters of the resnet model and...\nfor param in model.parameters():\n    param.requires_grad = False\n\n#Replace the resnet fully connected layer with two linear layers \nmodel.fc = nn.Sequential(\n               nn.Linear(2048, 128),\n               nn.ReLU(inplace=True),\n               nn.Linear(128, 2)).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n**Question 3.2:** What do you think `param.requires_grad = False` does? Which transfer learning techniques is being applied?","metadata":{}},{"cell_type":"markdown","source":"<font color=\"green\">  Write your answer here.\n</font>\n\n****","metadata":{}},{"cell_type":"code","source":"print(model.fc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remember that the `CrossEntropyLoss` function takes *logits* as input, which are the raw, unscaled outputs of the model (without softmax applied), as it applies softmax internally.","metadata":{}},{"cell_type":"code","source":"#Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n#Define the optimizer\noptimizer = optim.Adam(model.fc.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note that we can specify that the Adam optimizer will only optimize the parameters of the last fully connected layers. \n\nNow we write the training function.","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs=5):\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n\n        for phase in ['train', 'validation']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                _, preds = torch.max(outputs, 1)\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(image_datasets[phase])\n            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n\n            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n                                                        epoch_loss,\n                                                        epoch_acc))\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we train the last layers with our training dataset.","metadata":{}},{"cell_type":"code","source":"model_trained = train_model(model, criterion, optimizer, num_epochs=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we save the model in the Kaggle environment. After running the following cell, you will find the model in the Output section on the left menu. Note that Kaggle does not retain the output between sessions, so if you wish to reuse the trained model, you will need to download it or store it elsewhere.\n\nSpecifically, we save the model weights and biases in a dictionary.","metadata":{}},{"cell_type":"code","source":"#Save the model\ntorch.save(model_trained.state_dict(), 'model_weights.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To load a model with imported weights, we need to first define the model architecture, then we load the saved weights.","metadata":{}},{"cell_type":"code","source":"# Build the same architecture\nmodel = models.resnet50(pretrained=False).to(device)\nmodel.fc = nn.Sequential(\n               nn.Linear(2048, 128),\n               nn.ReLU(inplace=True),\n               nn.Linear(128, 2)).to(device)\n\n#Load the dictionary of saved parameters\nmodel.load_state_dict(torch.load('model_weights.h5'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 Evaluation\nNow, let's explore the model's results. We have already collected the training and validation loss and accuracy of the classifier. Since the class distribution is balanced, accuracy should serve as a reliable measure of the classifier's quality. To have a more concrete idea of our results, we will plot a few validation set images and check the model's predictions. This is always a good practice.","metadata":{}},{"cell_type":"code","source":"validation_img_paths = ['validation/alien/3.jpg',\n                       'validation/alien/11.jpg',\n                       'validation/alien/13.jpg',\n                        'validation/alien/87.jpg',\n                        'validation/predator/18.jpg',\n                       'validation/predator/49.jpg',\n                       'validation/predator/75.jpg',\n                        'validation/predator/91.jpg']\n\nvalidation_img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we build the input of the network. Remember that we need to apply the same transformation as before.","metadata":{}},{"cell_type":"code","source":"#Build the input for the network \nvalidation_batch = torch.stack([data_transforms['validation'](img).to(device)\n                                for img in validation_img_list])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n**Task 4.1:** Use the model to predict the probabilities assigned to all the images in the validation batch.\n****","metadata":{}},{"cell_type":"code","source":"#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\npred_probs = None\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\nprint (pred_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's build the labels and plot the images with corresponding class and probability assigned by the model.","metadata":{}},{"cell_type":"code","source":"# Move the tensor datat to cpu \npred_probs = pred_probs.cpu().data.numpy()\n\n#Define the classes by the higher probability\nclasses = np.argmax(pred_probs,axis=1)\n\n#Labels for the plot\nlabels = [] \n\nfor i,label in enumerate(classes):\n    if label == 0:\n        labels.append('alien prob '+ str(pred_probs[i][0])) \n    else:\n        labels.append('predator prob '+ str(pred_probs[i][1])) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_images(validation_img_list,labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**That's all for now, see you at the next lab!**","metadata":{}},{"cell_type":"markdown","source":"**Feedback Form**: please fill in the following form to provide feedback https://forms.office.com/e/zV2zVMgFxR ","metadata":{}}]}