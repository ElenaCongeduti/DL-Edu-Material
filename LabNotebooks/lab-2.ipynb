{"metadata":{"colab":{"provenance":[{"file_id":"1ttiWqf2a3bFB-FzC1yNvav0wqaKS0vLm","timestamp":1693393955506},{"file_id":"1SNxiok7jRinN2QeTeuFN1qADDVhky4a0","timestamp":1690873313789}]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Lab 2**\n\nTI3155TU Deep Learning (2024 - 2025)\n\nAuthors: Ali Alper Ataşoğlu, Elena Congeduti","metadata":{"id":"nVCW6YSA4jSH"}},{"cell_type":"markdown","source":"# Instructions\nWe recommend that you fork the lab notebooks by selecting `Copy & Edit` on the notebook's homepage. This will create a copy in your Kaggle repository, allowing you to work on it and save your progress as you go. Kaggle provides a pre-configured virtual environment, which means that most of the libraries we will use are already downloaded and ready to use. Therefore, you typically do **not** need to `pip install` additional resources.\n\nAlternatively, to work on Google Colab, you just need to select the `Open in Colab` option in the notebook's homepage menu. Finally, if you want to work locally, you will need to set up your own virtual environment. Check the Lab Instructions in [Learning Material](https://brightspace.tudelft.nl/d2l/le/content/682797/Home?itemIdentifier=D2L.LE.Content.ContentObject.ModuleCO-3812764) on Brightspace for detailed information on the virtual environment configuration.\n\nThese labs include programming exercises and insight questions. Follow the instructions in the notebook. Fill in the text blocks to answer the questions and write your own code to solve the programming tasks within the designated part of the code blocks:\n\n```python\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\n\n\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n```\n\nSolutions will be shared the week after the lab is published. Note that these labs are designed for practice and are therefore **ungraded**.","metadata":{"id":"7rSnXZSgqpfk"}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\n\nimport numpy as np\n\nfrom scipy.stats import multivariate_normal\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:08:21.469295Z","iopub.execute_input":"2024-10-25T14:08:21.470764Z","iopub.status.idle":"2024-10-25T14:08:21.477638Z","shell.execute_reply.started":"2024-10-25T14:08:21.470712Z","shell.execute_reply":"2024-10-25T14:08:21.476632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 Multi-layer Perceptrons\n\nModern neural networks consist of many building blocks that are assembled together to create deep and sophisticated architectures. Here we want to introduce you to the basic blocks that will allow you to build your first neural network.\n","metadata":{"id":"0wDALZseTH11"}},{"cell_type":"markdown","source":"## 1.1 Linear Layers\n\nA linear (fully connected) layer represents a linear transformation of an input $x \\in \\mathbb{R}^{N_{\\text{in}}}$ to an output $y  \\in \\mathbb{R}^{N_{\\text{out}}}$ described as:\n\n$$ y = x\\cdot\\mathbf{W}  + \\mathbf{b}$$\n\nfor weight $\\mathbf{W}\\in \\mathbb{R}^{N_{\\text{in}} \\times N_{\\text{out}}}$ and bias $\\mathbf{b}\\in \\mathbb{R}^{N_{\\text{out}}}$ matrices. $N_{\\text{in}}$ and $N_{\\text{out}}$ correspond to the dimension (or features) of the input and output space respectively.\n","metadata":{"id":"UBvhHSYfVhUB"}},{"cell_type":"markdown","source":"***\n**Question 1.1:** What does $\\textbf{W}_{ij}$ represent exactly?","metadata":{"id":"WapxJq9mqsMk"}},{"cell_type":"markdown","source":"<font color='green'>Write your answere here</font>\n****","metadata":{"id":"jUqezoTGrQRQ"}},{"cell_type":"markdown","source":"Moreover the forward pass formula above can also be used to process multiple inputs at the same time in a *batch* $x \\in \\mathbb{R}^{\\text{batch size}\\times N_{\\text{in}}}$, where *batch size* indicates the number of samples in the batch. This allows to parallelize the computations, reducing processing time significantly. Remember that in this case, broadcasting is used to sum the bias vector $b$ with the matrix product $x \\cdot\\textbf{W}$. This yields the same restuls as computing $y=x \\cdot\\textbf{W}+\\,\\textbf{1}\\cdot\\textbf{b}^T$, where $\\textbf{1}\\in\\mathbb{R}^{{\\text{batch size}}}$ is a vector whose components are all one and $\\textbf{1}\\cdot\\textbf{b}^T\\in \\mathbb{R}^{\\text{batch size}\\times N_{out}}$ is a matrix with each of its $\\text{batch size}$ rows equal to the bias $b$.\n\nTo build such a layer, we can complete the reference implementation provided below.","metadata":{"id":"iDc5uQ6UrOQ2"}},{"cell_type":"markdown","source":"****\n**Task 1.2:** Complete the code to initialize the $\\mathbf{W}$ and $\\mathbf{b}$ matrices with the correct dimensions, and compute the output $y$ of the linear layer.\n\n**Hint**: remember that the sum supports broadcasting.\n****","metadata":{"id":"MTp8O_6tHnVa"}},{"cell_type":"code","source":"class Linear (object):\n    \"\"\"\n    Fully connected layer.\n    \"\"\"\n\n    def __init__(self, N_in, N_out):\n        \"\"\"\n        Args:\n          N_in:  number of input features (input space dimension)\n          N_out: number of output features (output space dimension)\n        \"\"\"\n        #############################################################################\n        #                           START OF YOUR CODE                              #\n        #############################################################################\n        self.weight =None\n        self.bias =None\n        #############################################################################\n        #                            END OF YOUR CODE                               #\n        #############################################################################\n\n        # Initialize parameters\n        self.init_params()\n\n    def init_params(self):\n        \"\"\"\n        Initialize layer parameters by sampling from uniform distribution over [0,1)\n        \"\"\"\n        self.weight = torch.randn_like(self.weight)\n        self.bias = torch.rand_like(self.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of Linear layer: multiply input tensor by weights and add\n        bias.\n\n        Args:\n            x: input tensor\n\n        Returns:\n            y: output tensor\n        \"\"\"\n\n        #############################################################################\n        #                           START OF YOUR CODE                              #\n        #############################################################################\n\n        #############################################################################\n        #                            END OF YOUR CODE                               #\n        #############################################################################\n\n        return y","metadata":{"id":"oWcqUhVgfR5A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having defined a linear layer, we can now use it to perform a forward pass on some random input $x$.","metadata":{"id":"G_vad40qZzM4"}},{"cell_type":"markdown","source":"*****\n**Question 1.3:**\nThe input sample $x$ comprises 2 examples each with 3 dimensions (or features) and a linear layer has 4 units. What is the dimension of the output tensor $y$?","metadata":{"id":"w-IyNdTfhKn3"}},{"cell_type":"markdown","source":"<font color='green'>Write your answere here</font>\n\n****","metadata":{"id":"XeSzSt_8htKC"}},{"cell_type":"markdown","source":"****\n**Task 1.4:** Instantiate a linear layer with the variable name ```layer```. Perform a forward pass on the input $x$ and check the shape of the output $y$.\n****","metadata":{"id":"oMrcl6pCf1CT"}},{"cell_type":"code","source":"# Define layer dimensions and dummy input\nbatch_size, N_in, N_out = 2, 3, 4\n\n# Make input tensor has shape [batch_size, N_in] with random values between -1 and 1\nx = 2*torch.rand((batch_size, N_in))-1\n\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\ny= None\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\n# Shape output tensor y\nprint('Shape of ouput tensor y:', y.shape)","metadata":{"id":"bSALY1wPFGXZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to validate our solution and further explore the capabilities of PyTorch, we can solve the same exercise by using the building blocks provided by PyTorch.\n\nThe linear layer as well as many other useful classes are defined in the **torch.nn** module. A full list of these classes can be found [here](https://pytorch.org/docs/stable/nn.html).\n\nYou can compare the outputs of both implementations using [`torch.allclose`]( https://pytorch.org/docs/stable/generated/torch.allclose.html) which returns True if the tensors have all their components sufficiently \"close\" to each other (i.e. if the distance between the two tensors is lower than a small tolerance threshold).","metadata":{"id":"xB8dNwsqaVRn"}},{"cell_type":"markdown","source":"****\n**Task 1.5:** First check the documentation for the `Linear` function from `torch.nn` [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). Initialize a linear layer, compute `torch_y` as the result of the forward pass on the input $x$ and verify that the two outoputs `y` and `torch_y` concide using the function `torch.allclose`.\n****","metadata":{}},{"cell_type":"code","source":"#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\ntorch_layer = None\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\n# Load the parameters from our layer into the Pytorch layer\ntorch_layer.weight = torch.nn.Parameter(layer.weight.T)\ntorch_layer.bias = torch.nn.Parameter(layer.bias)\n\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\ntorch_y = None\noutputs_same = None\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\n# Shape of output tensor torch_y\nprint('Shape of ouput tensor torch_y:', torch_y.shape)\n\n#Are the output tensors the same?\nprint('Outputs identical: ', outputs_same)","metadata":{"id":"xa-_W-MgFJzN","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Your forward implementation of the linear layer is *correct* if `True` is returned.","metadata":{"id":"DlgJV-haboST"}},{"cell_type":"markdown","source":"## 1.2 Activation functions\n\nAt this point, you may wonder about the difference between a simple linear regression model and our linear layer, and the short answer is - there is none. However, we aim for our neural network to learn non-linear relationships between the input and output spaces. This is where non-linear activation functions come into play.","metadata":{"id":"Z6mutfapbsLY"}},{"cell_type":"markdown","source":"One of the most common activation function is the Rectified Linear Unit (ReLU) defined as\n\n$$\n\\operatorname{ReLU}(x) =\n\\begin{cases}\nx & \\text{if } x\\geq 0\\\\\n0 & \\text{if } x < 0\n\\end{cases}\n\\quad= \\quad\\max\\{0, x\\}\n$$","metadata":{"id":"NFoz2CugGCCq"}},{"cell_type":"code","source":"class ReLU (object):\n    \"\"\"\n    ReLU non-linear activation function.\n    \"\"\"\n    def __init__(self):\n        super(ReLU, self).__init__()\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of ReLU non-linear activation function: y=max(0,x).\n        Args:\n            x: input tensor\n\n        Returns:\n            y: output tensor\n        \"\"\"\n        #Clamp all the values below zero and replace them with 0\n        y = torch.clamp(x,min=0)\n\n        return y","metadata":{"id":"28920vtRHhlp","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's now test our implementation on the dummy input tensor $x$.","metadata":{"id":"ZfOlo_HtVtOM"}},{"cell_type":"code","source":"print('Input tensor x\\n:',x)\n\n#Our implementation\nrelu = ReLU()\ny_relu = relu.forward(x)\n\nprint('\\nOutput of our ReLU implementation:\\n',y_relu)","metadata":{"id":"WvQhkUyKOJYy","executionInfo":{"status":"ok","timestamp":1693759365493,"user_tz":-120,"elapsed":236,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"38c57c00-b571-4c2c-ca37-1f473e31483b","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also apply ReLU in PyTorch using the `torch.nn.ReLU` class.","metadata":{"id":"DmFMFtddNdKI"}},{"cell_type":"markdown","source":"****\n**Task 1.6:** Check the documentation of `torch.nn.ReLU` then use it to initialize a ReLU function and perform a forward pass on the input $x$.\n****","metadata":{}},{"cell_type":"code","source":"#Torch implementation\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\ntorch_y_relu = None\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\ncheck = torch.allclose(torch_y_relu,y_relu)\nprint('\\nOutputs identical: ', check)","metadata":{"id":"O9nc28KMVN7J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n**Task 1.7:**\nImplement your definition of the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function). Then verify your implementation by comparing it with the PyTorch counterpart `torch.nn.sigmoid`.\n****","metadata":{"id":"Z9-9sfe-HI9r"}},{"cell_type":"code","source":"class Sigmoid (object):\n    \"\"\"\n    Sigmoid non-linear activation function.\n    \"\"\"\n    def forward(self, x):\n        \"\"\"\n        Forward pass of Sigmoid non-linear activation function: y=1/(1+exp(-x)).\n\n        Args:\n            x: input tensor\n\n        Returns:\n            y: output tensor\n        \"\"\"\n        ########################################################################\n        #                        START OF YOUR CODE                            #\n        ########################################################################\n\n        ########################################################################\n        #                         END OF YOUR CODE                             #\n        ########################################################################\n\n        return y","metadata":{"id":"YOm4JCVSG8Mp","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Our implementation\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\n\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\n#Torch implementation\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\n\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\ncheck = torch.allclose(torch_y_sigma,y_sigma)\n\nprint('\\nOutputs identical: ', check)","metadata":{"id":"wdUetRpxW4Bk","executionInfo":{"status":"ok","timestamp":1693759372352,"user_tz":-120,"elapsed":210,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"a95cb67e-2db0-4610-e571-61a5f08126c9","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n**Question 1.8:** What is the shape of the output tensor $y$ for an input tensor $x\\in\\mathbb{R}^{\\text{batch size}\\times N_{out}}$?","metadata":{"id":"y1shBPgcYKO2"}},{"cell_type":"markdown","source":"<font color='green'>Write your answere here</font>\n****","metadata":{"id":"pLmM-w4MYxJB"}},{"cell_type":"markdown","source":"A list of all available non-linearities in PyTorch can be found [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).","metadata":{"id":"7hiL-1zwRh_G"}},{"cell_type":"markdown","source":"Despite similarities in implementation, these two activation functions—and activation functions in general—have very different  behaviors.Therefore, we recommand to carefully consider your choices, review documentation or other suggestions and warnings regarding the use of specific activation functions. ","metadata":{"id":"7ipKU6x8dshy"}},{"cell_type":"markdown","source":"## 1.3 Building a network model\n\nUsing the linear layer and the activation functions described beforehand, we can start piecing together our first neural network. To do so, we can define a base class that will allow us to stack an arbitrary amount of layers and activation functions.","metadata":{"id":"FTvLIxmgeIi2"}},{"cell_type":"markdown","source":"****\n**Task 1.9:** Complete the code, performing a forward pass through all the layers in the net.\n****","metadata":{"id":"dR9gnTUXNNWP"}},{"cell_type":"code","source":"class Net (object):\n    \"\"\"\n    Neural network object containing layers.\n    \"\"\"\n\n    def __init__(self, layers):\n        \"\"\"\n        Args:\n          layers: list of layers in neural network\n        \"\"\"\n        self.layers = layers\n\n    def forward(self, x):\n        \"\"\"\n        Performs forward pass through all layers of the network.\n\n        Args:\n            x: input tensor\n\n        Returns:\n            x: output tensor\n        \"\"\"\n\n        ########################################################################\n        #                        START OF YOUR CODE                            #\n        ########################################################################\n        x = None\n        ########################################################################\n        #                         END OF YOUR CODE                             #\n        ########################################################################\n\n        return x","metadata":{"id":"vi8luwCMgobr","execution":{"iopub.status.busy":"2024-10-25T13:43:57.181915Z","iopub.execute_input":"2024-10-25T13:43:57.182568Z","iopub.status.idle":"2024-10-25T13:43:57.190536Z","shell.execute_reply.started":"2024-10-25T13:43:57.182518Z","shell.execute_reply":"2024-10-25T13:43:57.189240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an example, now we can defined a two layer network with an input size of 2, 5 nodes in the hidden layer and an output size of 1.","metadata":{"id":"mOPHaurqi-uC"}},{"cell_type":"code","source":"N_in = 3\nhidden_dim = 5\nN_out = 1\n\n# Define and initialize layers\nlayers = [Linear(N_in, hidden_dim),\n          ReLU(),\n          Linear(hidden_dim, N_out)]\n\n# Initialize network\nnet = Net(layers)\n\n# Do forward pass\ny = net.forward(x)\n\n# What will be the shape of output tensor y?\nprint('Shape of ouput tensor y:', y.shape)","metadata":{"id":"5k4uPheAjLsY","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n**Question 1.10**: Depict the computational graph for this network.\nWhat do the dimesions of the output correspond to?","metadata":{"id":"z_valLuGQLoW"}},{"cell_type":"markdown","source":"<font color='green'>Write your answere here</font>\n****\n","metadata":{"id":"B_cAC7_9bBXN"}},{"cell_type":"markdown","source":"We will now create the same neural network in PyTorch. PyTorch uses the `nn.Module` base class for neural network architectures, which is similar to the `Net` object that we have just defined. However, other than in the `Net` class, you have to define all layers inside the network definition.\n\nThis is an important exercise, as this is how you will define all your future models in PyTorch.\n\nYou can print a PyTorch `module` to see all sub-modules (i.e. layers) in the module.","metadata":{"id":"bEM0rh9IRJel"}},{"cell_type":"markdown","source":"****\n**Task 1.11:** Complete the code, defining the same layers as we did in our network class with the attribute names ```layer1``` and ```layer2``` and using the activation function ```relu```. Implement the forward pass and initialize a TorchNet object as ```torch_net```.\n****","metadata":{"id":"W4iE0gxkgZYc"}},{"cell_type":"code","source":"class TorchNet(nn.Module):\n    \"\"\"\n    PyTorch neural network. Network layers are defined in __init__ and forward\n    pass implemented in forward.\n    \"\"\"\n\n    def __init__(self, N_in, hidden_dim, N_out):\n        \"\"\"\n        Args:\n          N_in: number of features in input layer\n          hidden_dim: number of features in hidden layer\n          N_out: number of features in output layer\n        \"\"\"\n\n        super(TorchNet, self).__init__()\n\n        ########################################################################\n        #                        START OF YOUR CODE                            #\n        ########################################################################\n\n        ########################################################################\n        #                         END OF YOUR CODE                             #\n        ########################################################################\n\n    def forward(self, x):\n        \"\"\"\n        Performs forward pass through all layers of the network.\n\n        Args:\n            x: input tensor\n\n        Returns:\n            x: output tensor\n        \"\"\"\n\n        ########################################################################\n        #                        START OF YOUR CODE                            #\n        ########################################################################\n        x = None\n        ########################################################################\n        #                         END OF YOUR CODE                             #\n        ########################################################################\n        return x\n\n","metadata":{"id":"Mln6iWyrP1NF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Pytorch network\n########################################################################\n#                        START OF YOUR CODE                            #\n########################################################################\ntorch_net = None\n########################################################################\n#                         END OF YOUR CODE                             #\n########################################################################\n\nprint(torch_net)","metadata":{"id":"gH8xe0vvThH8","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To check your implementation we will now again compare the outputs of the two networks. For that, we need to load the weights from our network into the PyTorch network.","metadata":{"id":"Lvn0LqdZT7-u"}},{"cell_type":"code","source":"# Load the parameters from our model into the Pytorch model\ntorch_net.layer1.weight = nn.Parameter(net.layers[0].weight.t()) # transpose weight by .t()\ntorch_net.layer1.bias = nn.Parameter(net.layers[0].bias)\ntorch_net.layer2.weight = nn.Parameter(net.layers[2].weight.t()) # transpose weight by .t()\ntorch_net.layer2.bias = nn.Parameter(net.layers[2].bias)\n\n# Perform forward pass\ntorch_y = torch_net(x)\n\n# What will be the shape of output tensor torch_y?\nprint('Shape of ouput tensor y:', torch_y.shape)\n\n# Compare outputs using torch.allclose\noutputs_same = torch.allclose(y, torch_y)\nprint('Network outputs identical: ', outputs_same)","metadata":{"id":"wLxQ7mbEUHhU","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each `nn.Module` in PyTorch has a default initialization method for its parameters. Since initialization plays a critical role in the network's learning, we will implement a custom initialization method that can also be called whenever we want to reset the network. We will sample the initial parameters from a Gaussian distribution around zero with small variance.","metadata":{}},{"cell_type":"code","source":"class TorchNet1(nn.Module):\n    \"\"\"\n    PyTorch neural network. Network layers are defined in __init__ and forward\n    pass implemented in forward.\n    \"\"\"\n\n    def __init__(self, N_in, N_out):\n        \"\"\"\n        Args:\n          N_in: number of features in input layer\n          hidden_dim: number of features in hidden layer\n          N_out: number of features in output layer\n         \"\"\"\n        \n        super(TorchNet1, self).__init__()\n\n        self.layer = nn.Linear(N_in, N_out)\n        self.relu = nn.ReLU()\n\n        self.reset_params()\n        \n    def reset_params(self, mean=0.0, std=0.02):\n        \"\"\"\n        Initializes the parameters of the network\n        \"\"\"\n        nn.init.normal_(self.layer.weight, mean=mean, std=std)\n        nn.init.constant_(self.layer.bias, 0.0)\n    \n    def forward(self, x):\n        \"\"\"\n        Performs forward pass through all layers of the network.\n\n        Args:\n            x: input tensor\n\n        Returns:\n            x: output tensor\n        \"\"\"\n\n        x = self.layer1(x)\n        x = self.relu(x)\n\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's have a look at the parameter values and then reset them. To access the entire collection of learnable parameters, you can use the `nn.Module` method `.parameters()`.","metadata":{}},{"cell_type":"markdown","source":"****\n**Task 1.12:** Reset the weights of the linear layer of the network below using mean 0 and standard deviation 1 and then print them.\n(Hint: loop over `net1.parameters()` to access and print them all)\n****","metadata":{}},{"cell_type":"code","source":"#Initialize a network\nnet1 = TorchNet1(2,1)\n\n# Initial values of the parameters\nprint('Initial values of the network weights', net1.layer.weight.data)\nprint('Initial values of the network bias', net1.layer.bias.data)\n\n########################################################################\n#                        START OF YOUR CODE                            #\n########################################################################\n\n########################################################################\n#                         END OF YOUR CODE                             #\n########################################################################","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Introducing a regression problem\n\nFinally, we have gathered all the necessary tools to solve our first problem using a neural network.\n\nIn many areas of engineering, neural networks are used to approximate complex non-linear models when these models are either missing an analytical formulation, or are too computationally expensive to evaluate.\n\nStarting off simple, the function we will try to approximate is the 2-dimensional Gaussian distribution:\n\n$$\nf(x, y)=\\frac{1}{2 \\pi \\sigma^2} e^{-\\left[\\left(x-\\mu_x\\right)^2+\\left(y-\\mu_y\\right)^2\\right] /\\left(2 \\sigma^2\\right)}\n$$\n\nwith $\\sigma = 50$, $\\mu_{x} = 0$, and $\\mu_{y} = 0.0$.\n\n","metadata":{"id":"pZI6PIciksTh"}},{"cell_type":"markdown","source":"To do so, first we will start by generating a dataset that we will use to evaluate our network on. This will be done by sampling the function on a square grid ranging from -2 to 2.","metadata":{"id":"HU1Dwaw_FNHK"}},{"cell_type":"code","source":"# Define the gaussian distribution function\nmux = 0.\nmuy = 0.\nsigma = 50\nmean = [mux, muy]\ncov = [[sigma, 0.0], [0.0, sigma]]\nvar = multivariate_normal(mean=mean, cov=cov)\n\n# Number of grid points per axis\nnum_sample_points = 50\n\n# Generate range of x and y values\nx_range = np.linspace(-2, 2., num_sample_points)\ny_range = np.linspace(-2, 2., num_sample_points)\n\n# Generate grid coordinates that will be used to evaluate the gaussian on\nx_nodes, y_nodes = np.meshgrid(x_range, y_range)\nnodes = np.column_stack((x_nodes.ravel(), y_nodes.ravel()))\n\n# Compute the function over the grid coordinates and store the values\nground_truth = np.zeros(len(nodes), dtype=np.float32)\nfor i, node in enumerate(nodes):\n  gauss_value = var.pdf(node)\n  ground_truth[i] = gauss_value\n\n# Convert to numpy matrix to tensor\nground_truth = torch.Tensor(ground_truth)\n","metadata":{"id":"InDWm6zxoTkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And with this we have our dataset! The 2D-grid input points are stored in the variable `nodes`, while the corresponding gaussian values are stored in `ground_truth`. We can visualize the dataset using `matplotlib` functionalities.","metadata":{"id":"_vKDGK9lL7uf"}},{"cell_type":"code","source":"def plot_result_surface(x_nodes, y_nodes, z_values):\n  if isinstance(z_values, torch.Tensor):\n    z_values = z_values.detach().numpy()\n\n  fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n\n  # Plot the surface.\n  surf = ax.plot_surface(x_nodes, y_nodes, np.reshape(z_values, x_nodes.shape), cmap=cm.coolwarm,\n                        linewidth=0, antialiased=False)\n\n  plt.show()\n\nplot_result_surface(x_nodes, y_nodes, ground_truth)","metadata":{"id":"yy7q3OQiD_uy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nTo approximate the gaussian distribution, we will use a 2-layer network with hidden dimension of 25.","metadata":{"id":"Vb2TrVIxMbZO"}},{"cell_type":"markdown","source":"****\n**Question 2.1**: What should be the number of features of the input and output layer for this problem?","metadata":{"id":"u3bBSzarfJKS"}},{"cell_type":"markdown","source":"<font color='green'>Write your answere here</font>\n****\n","metadata":{"id":"YvMxoEoMfJKT"}},{"cell_type":"markdown","source":"****\n**Task 2.2:** Define a PyTorch Net with the dimensions mentioned above and evalute it on the dataset.\n*****","metadata":{"id":"2OEFwkSEeWFC"}},{"cell_type":"code","source":"#Set the torch seed for reproducibility\ntorch.manual_seed(999)\n\n# Initialize Pytorch network\n########################################################################\n#                        START OF YOUR CODE                            #\n########################################################################\nnet = None\n########################################################################\n#                         END OF YOUR CODE                             #\n########################################################################\n\n\n# Convert the numpy matrices to PyTorch tensors\ntorch_nodes = torch.Tensor(nodes)\n\n# Evaluate the network on the grid - Perform forward pass\n########################################################################\n#                        START OF YOUR CODE                            #\n########################################################################\nprediction=None\n########################################################################\n#                         END OF YOUR CODE                             #\n########################################################################\n\nprint('Shape of the output\\n:', prediction.shape)\n\n# Plot the network's prediction\nplot_result_surface(x_nodes, y_nodes, prediction)","metadata":{"id":"XRM8dK-_NXmV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Disappointingly, the results are far from providing a good approximation of the gaussian function. That is because we have not trained the network yet but we are only looking at the outcome of a random initialization of the weights and bias. Without proper training, neural networks have very poor performance, regardless of the task.\n\nMoreover, currently we have no way of addressing how bad our network is at approximating the target function. Therefore, we need to come up with a systematic method of quantifying the error between the ground truth and the network's prediction.\n","metadata":{"id":"l10FiTnUPiYD"}},{"cell_type":"markdown","source":"# 3 Loss functions\n\nLoss functions provide exactly this functionality, i.e. they quantify the error between the ground truth and the prediction. There are many types of loss functions with different properties and suitable for different kind of problems. The most commonly used loss function for regression problems is the **Mean-Square error** defined as:\n\n$$\n\\mathrm{MSE}=\\frac{1}{\\text{n}} \\sum_{i=1}^n\\left(y_i-\\hat{y}_i) \\right)^2\n$$\n\nwhere $y_{i}$ is the observed data, $\\hat{y}_{i}$ is the prediction and $n$ the number of samples.\n\nPyTorch provides definitions of a myriad of loss functions, again saving some time for the user (check the documentation [here](https://pytorch.org/docs/stable/nn.html#loss-functions)).\n\nNow we can use the MSE loss to measure how poorly our network is approximating the Gaussian. As usual, we will first define our own version and then compare it with the PyTorch implementation.\n","metadata":{"id":"7mAKMpTI4XnR"}},{"cell_type":"markdown","source":"****\n**Task 3.1:** Define the a function computing the MSE loss between the ground truth and the predicted values. \n****","metadata":{}},{"cell_type":"code","source":"def MSE_loss (y,hat_y):\n    ########################################################################\n    #                        START OF YOUR CODE                            #\n    ########################################################################\n    loss = None\n    ########################################################################\n    #                         END OF YOUR CODE                             #\n    ########################################################################\n    return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we verify our implementation with the PyTorch function.","metadata":{}},{"cell_type":"code","source":"# Define MSE loss function\nMSE_loss = torch.nn.MSELoss()\n\n# Compute MSE over the ground truth and network prediction\ntorch_loss = MSE_loss.forward(ground_truth, prediction).item()\nloss = MSE_loss(ground_truth, prediction)\n\nprint(f\"Average loss over whole dataset: {torch_loss:.6f}\")\nprint(f\"MSE outputs identical: {loss== torch_loss}\")","metadata":{"id":"UIL_MU3M6hS2","executionInfo":{"status":"ok","timestamp":1693759639946,"user_tz":-120,"elapsed":218,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"ec5ff815-4461-4692-8879-c4d8bc40125f","_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if we can improve that using a simple training procedure.\n\n","metadata":{"id":"dFuqGXC66hnl"}},{"cell_type":"markdown","source":"# 4 First steps toward training\nWe can work our way through a simple training loop in which the network parameters are randomly updated whenever the network performs worse than a desired threshold level for the loss function. That means we will repeat the following training steps:\n\n1.   Compute forward pass of the network.\n2.   Compute loss between prediction and ground truth values.\n3.   If the loss is below a certain threshold, stop training.\n4.   Otherwise, randomly update the network's parameters.\n","metadata":{"id":"KelEfv0w6iPw"}},{"cell_type":"markdown","source":"****\n**Task 4.1:** Complete the code below to implement the training loop.\n****","metadata":{"id":"9ZeIVgVzn-ak"}},{"cell_type":"code","source":"# Define the error threshhold\nepsilon = 1e-5\n\n# Maximum number of training iterations\niter_num = 0.\nmax_iters = 50\n\nwhile loss > epsilon and iter_num < max_iters:\n  iter_num += 1\n\n  # Update network's weights and biases by drawing from a normal distributions\n  for layer in net.children():\n    if hasattr(layer, 'reset_parameters'):\n        layer.weight = torch.nn.Parameter(0.025 * (-2 * torch.rand(layer.weight.shape) + 1))\n        layer.bias = torch.nn.Parameter(0.05 * (-2 * torch.rand(layer.bias.shape) + 1))\n\n    # Evaluate the network on the grid\n    ########################################################################\n    #                        START OF YOUR CODE                            #\n    ########################################################################\n    prediction = None\n    ########################################################################\n    #                         END OF YOUR CODE                             #\n    ########################################################################\n\n    # Compute the loss\n    ########################################################################\n    #                        START OF YOUR CODE                            #\n    ########################################################################\n    loss = None\n    ########################################################################\n    #                         END OF YOUR CODE                             #\n    ########################################################################\n\n  print(f\"Iteration: {iter_num}, Loss: {loss:.8f}\")\n\n# Plot the network's prediction\nplot_result_surface(x_nodes, y_nodes, prediction)","metadata":{"id":"5tSOrobV9NVz","_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the result is still not perfect, we can see that is much better than the initial one, even without applying any smart learning rule.\n\nIn the next labs, we will see how to implement learning algorithms to update the network parameters that will make the networks coverge much faster and achieve substantially better results.\n","metadata":{"id":"uOorYgHMnbBu"}},{"cell_type":"markdown","source":"**That's all for now, see you in the next lab!**","metadata":{"id":"757s0YUzo9gu"}},{"cell_type":"markdown","source":"**Feedback Form:** please fill in the following form to provide feedback https://forms.office.com/e/zFR5Nx0NFC","metadata":{}}]}