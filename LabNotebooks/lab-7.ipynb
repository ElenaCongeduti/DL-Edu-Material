{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1vu_pIE6wcHStyvgH4brKCZhPr_ErtcCp","timestamp":1696084673558}],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Lab 7**\n\nTI3155TU Deep Learning (2024 - 2025)\n\nAdapted by Elena Congeduti from TU Delft CS4240 Deep Learning course.\n","metadata":{"id":"nVCW6YSA4jSH"}},{"cell_type":"markdown","source":"# Instructions\n**For this lab, we recommend working on Google Colab as it provides direct support for the TensorBoard library. To do this, select the 'Open in Colab' option from the notebook's homepage menu.**\n\nAlternatively, you can work locally. In this case, you will need to set up your own virtual environment. Check the Lab Instructions in [Learning Material](https://brightspace.tudelft.nl/d2l/le/content/682797/Home?itemIdentifier=D2L.LE.Content.ContentObject.ModuleCO-3812764) on Brightspace for detailed information on the virtual environment configuration.\n\nThese labs include programming exercises and insight questions. Follow the instructions in the notebook. Fill in the text blocks to answer the questions and write your own code to solve the programming tasks within the designated part of the code blocks:\n\n```python\n#############################################################################\n#                           START OF YOUR CODE                              #\n#############################################################################\n\n\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n```\n\nSolutions will be shared the week after the lab is published. Note that these labs are designed for practice and are therefore **ungraded**.","metadata":{"id":"7rSnXZSgqpfk"}},{"cell_type":"code","source":"# Setup\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\nimport math\n\n!pip install torchsummary\nfrom torchsummary import summary\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Additional Setup to use MNIST1D\n!pip install mnist1d\n\nimport mnist1d\nfrom mnist1d.data import get_templates, get_dataset_args, get_dataset\nfrom mnist1d.utils import set_seed, plot_signals, ObjectView, from_pickle\n\n# Additional Setup to use Tensorboard\n!pip install -q tensorflow\n%load_ext tensorboard","metadata":{"id":"gIvAOzFjhtw6","executionInfo":{"status":"ok","timestamp":1698084425722,"user_tz":-120,"elapsed":7085,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"89cda093-f1c3-48be-dea9-3c4de05bf341","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:59:35.820815Z","iopub.execute_input":"2024-11-27T13:59:35.821281Z","iopub.status.idle":"2024-11-27T14:00:05.263034Z","shell.execute_reply.started":"2024-11-27T13:59:35.821244Z","shell.execute_reply":"2024-11-27T14:00:05.261565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1 MNIST-1D Classification\n\nTo better see the effect of overfitting and different regularization techniques we will be using again the MNIST-1D dataset since it is very lightweight and can be easily trained without a GPU. You can find more information on MNIST-1D [here](https://greydanus.github.io/2020/12/01/scaling-down/).\n\n\n\nUnlike regular MNIST, which consists of two-dimensional grayscale images, MNIST-1D contains one-dimensional waveforms which are automatically generated from a set of 10 templates. Run the following cell to get an overview of the different labels and the corresponding 1D waveform templates in MNIST-1D.","metadata":{"id":"7YX7M-1siKSl"}},{"cell_type":"code","source":"#Get templates of numbers in the dataset\ntemplates = get_templates()\n\n#Plot the different templates\nprint(\"Templates for the MNIST-1D dataset:\")\nfig = plot_signals(templates['x'], templates['t'], labels=templates['y'], ratio=1.33, dark_mode=False)","metadata":{"id":"SKyoycOgTAVL","executionInfo":{"status":"ok","timestamp":1698084426437,"user_tz":-120,"elapsed":720,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"40c95611-9e42-49aa-9b40-828ac59f421d","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:00:09.481387Z","iopub.execute_input":"2024-11-27T14:00:09.481835Z","iopub.status.idle":"2024-11-27T14:00:10.031882Z","shell.execute_reply.started":"2024-11-27T14:00:09.481795Z","shell.execute_reply":"2024-11-27T14:00:10.030834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will now load and preprocess the dataset, and display a sample for each of the 10 classes. To understand how the MNIST-1D shapes have been generated from the templates you can take a look at [this video](https://greydanus.github.io/assets/scaling-down/construction.mp4).","metadata":{"id":"JPkX8h2Fiuml"}},{"cell_type":"code","source":"# Load dataset\ndata = get_dataset(get_dataset_args(), path='./mnist1d_data.pkl',\n\n                   download=False, regenerate=True)\n\n\n# Set the batch size for training & testing\nb_size = 100\n\n# Convert 1D MNIST data to pytorch tensors\ntensors_train = torch.Tensor(data['x']), torch.Tensor(data['y']).long()\ntensors_test = torch.Tensor(data['x_test']),torch.Tensor(data['y_test']).long()\n\n# Create training set and test set from tensors\ntrain_set = TensorDataset(*tensors_train)\ntest_set = TensorDataset(*tensors_test)\n\n# Create dataloaders from the training and test set for easier iteration over the data\ntrain_loader = DataLoader(train_set, batch_size=b_size)\ntest_loader = DataLoader(test_set, batch_size=b_size)\n\n# Get some data and check for dimensions\ninput,label = next(iter(train_loader))\n\n# Check whether the data has the right dimensions\nassert(input.shape == torch.Size([b_size, 40]))\nassert(label.shape == torch.Size([b_size]))\n\n# Display samples from dataset\nfig = plot_signals(templates['x'], templates['t'], labels=templates['y'],\n                   args=get_dataset_args(), ratio=2.2, do_transform=True)","metadata":{"id":"bBB_2cn5bX8k","executionInfo":{"status":"ok","timestamp":1698084446086,"user_tz":-120,"elapsed":12191,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"48f1d27a-a981-4baa-b42b-ab1d5115d121","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:01:23.547691Z","iopub.execute_input":"2024-11-27T14:01:23.548133Z","iopub.status.idle":"2024-11-27T14:01:26.953558Z","shell.execute_reply.started":"2024-11-27T14:01:23.548095Z","shell.execute_reply":"2024-11-27T14:01:26.952464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us now train a simple (but rather large) fully connected network to classify the MNIST-1D samples. In the forward pass you see that the input of a layer is summed to its output, i.e. `h = h + F.relu(self.fc(h))`. These represent *residual connections* [[paper](https://arxiv.org/abs/1512.03385)] and are introduced to mitigate the problem of vanishing gradient for very deep networks. The network is defined as follows:","metadata":{"id":"7ihH5jIWGQxg"}},{"cell_type":"code","source":"class FCNet(nn.Module):\n\n    \"\"\"\n    Simple fully connected neural network with residual connections in PyTorch.\n    Layers are defined in __init__ and forward pass implemented in forward.\n    \"\"\"\n\n    def __init__(self):\n        \n        super(FCNet, self).__init__()\n\n        self.fc1 = nn.Linear(40, 500)\n        self.fc2 = nn.Linear(500, 500)\n        self.fc3 = nn.Linear(500, 500)\n        self.fc4 = nn.Linear(500, 500)\n        self.fc5 = nn.Linear(500, 500)\n        self.fc6 = nn.Linear(500, 10)\n\n    def forward(self, x):\n\n        h = F.relu(self.fc1(x))\n        h = h + F.relu(self.fc2(h))\n        h = h + F.relu(self.fc3(h))\n        h = h + F.relu(self.fc4(h))\n        h = h + F.relu(self.fc5(h))\n\n        return self.fc6(h)\n\n# Print network architecture using torchsummary\nsummary(FCNet(), (40,), device='cpu')","metadata":{"id":"FVCzc80Tx-3o","executionInfo":{"status":"ok","timestamp":1698084446087,"user_tz":-120,"elapsed":7,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"bebc534c-f5be-4e80-b3d4-e1e6857d9968","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:02:05.644275Z","iopub.execute_input":"2024-11-27T14:02:05.644681Z","iopub.status.idle":"2024-11-27T14:02:05.668727Z","shell.execute_reply.started":"2024-11-27T14:02:05.644648Z","shell.execute_reply":"2024-11-27T14:02:05.667485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It is a good common practice to define separate functions for the training and validation steps inside an epoch. These functions are then called once per epoch during training, which significantly cleans up the code for the training loop.","metadata":{"id":"S3RAFMvRM6Je"}},{"cell_type":"code","source":"def train(train_loader, net, optimizer, criterion):\n\n    \"\"\"\n    Trains network for one epoch in batches.\n\n    Args:\n\n        train_loader: Data loader for training set.\n        net: Neural network model.\n        optimizer: Optimizer (e.g. SGD).\n        criterion: Loss function (e.g. cross-entropy loss).\n\n    \"\"\"\n\n    avg_loss = 0\n    correct = 0\n    total = 0\n\n    # iterate through batches\n    for i, data in enumerate(train_loader):\n\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = net(inputs)\n        \n        #evaluate loss \n        loss = criterion(outputs, labels)\n        \n        #backward pass\n        #Computing derivatives\n        loss.backward()\n        \n        #update step\n        optimizer.step()\n\n        # keep track of loss and accuracy\n        avg_loss += loss\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    return avg_loss/len(train_loader), 100 * correct / total\n\ndef test(test_loader, net, criterion):\n\n    \"\"\"\n    Evaluates network in batches.\n    Args:\n\n        test_loader: Data loader for test set.\n        net: Neural network model.\n        criterion: Loss function (e.g. cross-entropy loss).\n    \"\"\"\n\n    avg_loss = 0\n    correct = 0\n    total = 0\n\n    # Use torch.no_grad to skip gradient calculation, not needed for evaluation\n    with torch.no_grad():\n\n        # iterate through batches\n        for data in test_loader:\n\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n\n            # forward pass\n            outputs = net(inputs)\n            \n            #evaluate loss\n            loss = criterion(outputs, labels)\n\n            # keep track of loss and accuracy\n            avg_loss += loss\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return avg_loss/len(test_loader), 100 * correct / total","metadata":{"id":"4PGWLMmodNjC","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:05:07.821361Z","iopub.execute_input":"2024-11-27T14:05:07.822159Z","iopub.status.idle":"2024-11-27T14:05:07.833216Z","shell.execute_reply.started":"2024-11-27T14:05:07.822075Z","shell.execute_reply":"2024-11-27T14:05:07.83206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have already seen how Tensorboard can provide powerfull visualizations. In the following exercise you will use it to log and visualize the training curves of your network.\n\n\n\nThe following cell will train your network for 100 epochs and write the results per epoch to tensorboard (this might take a few seconds).","metadata":{"id":"N_-cpXWsL2GP"}},{"cell_type":"code","source":"#Create a writer to write to Tensorboard\nwriter = SummaryWriter()\n\n#Set the number of epochs to for training\nepochs = 100\n\n# Create instance of Network\nnet = FCNet()\n\n# Create loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=5e-1)\n\nfor epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n\n    # Train on data\n    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n\n    # Test on data\n    test_loss, test_acc = test(test_loader,net,criterion)\n\n    # Write metrics to Tensorboard\n    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Test':test_loss}, epoch)\n    writer.add_scalars('Accuracy', {'Train': train_acc,'Test':test_acc} , epoch)\n\nprint('\\n Finished Training')\nwriter.flush()\nwriter.close()","metadata":{"id":"xFfjWk8_g5QH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note: if you are running this notebook on Kaggle you will have to write your own visualization functions to plot the training curves as Tensorboad is not directly supported. Otherwise switch to Google Colab as suggested in the initial instructions.  ","metadata":{}},{"cell_type":"markdown","source":"Now that the training is done successfully you can view the training curves of your network in tensorboard. It might take a few seconds until it is loaded.\n\n\n\nIf you later run another experiment and want to also see those results click the refresh icon in the top right corner. ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAaCAYAAACkVDyJAAABOElEQVRIDe2UsQ3CMBBFMwIjMAIbsBglJRI0dEgwCg2UwAZQQQkNpIgSox/pRx+Ls2OEIgosRXe2z/fy7bOz+yhzXX5ZlzCwfhf4mPRdsZm68rx3bPAxhjndqWK3eunrXCuFxXZGhmkBRmLA0BSifhRYXg4NBMny+aBJBp8ABFX5tYlViPpBIJVV+e0FpAngA+w3P4Z9E4hzYVNVXKhWVXKNzqtvAqkOVhf4/jsYoH4c+yaQZxdSZ8E+Asa2hn+cak2FnQPbbGmqOsSbCtsWjUJ5pqFCM4Ep1wJQvYv+U6c/ZQIRRJV4QULVijm+MiF1yBkEIoBniSIqdssG/Bj3XL4Y1mMssPK4Nu8fVUaBqpSJ39mYsiQggnEuSKqKq+upHgudGUG0rRQy+Bv2D4xWXeo2/7f061v6BP5uhjxw+N1/AAAAAElFTkSuQmCC)","metadata":{"id":"rRxZDo6UNPes"}},{"cell_type":"code","source":"# For Google Colab users only, local users run the next cell\n# Run Tensorboard\n\n%tensorboard --logdir runs/","metadata":{"id":"Hny0DB6T0izu","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:07:42.470387Z","iopub.execute_input":"2024-11-27T14:07:42.470794Z","iopub.status.idle":"2024-11-27T14:07:42.484123Z","shell.execute_reply.started":"2024-11-27T14:07:42.470761Z","shell.execute_reply":"2024-11-27T14:07:42.482811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For local users only\n# Uncomment the last line\n# run this cell once and wait for it to time out\n# run this cell a second time and you should see the board\n\n#%tensorboard --logdir runs/ --host localhost","metadata":{"id":"GyUv5nWK_hKz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To completely clean your tensorboard uncomment and run the following command.\n!rm -r runs","metadata":{"id":"W54Z3fyU6FZG"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Question 1.1:**\n\nCan you discuss the training curves and the accuracy progression?\n","metadata":{"id":"UwMuNI7-gKu_"}},{"cell_type":"markdown","source":"<font color='green'> Write your answer here\n</font>\n\n****","metadata":{"id":"IOn-evsugWvQ"}},{"cell_type":"markdown","source":"****\n\n**Question 1.2:**\n\nWould early stopping help to improve the performance of the model?\n","metadata":{"id":"yMRaLb_Ah9b4"}},{"cell_type":"markdown","source":"<font color='green'> Write your answer here </font>\n****","metadata":{"id":"BSikMbdriG2v"}},{"cell_type":"markdown","source":"However, drawing conclusions from a single experiment run can be misleading as the results can be heavily influenced by the randomness in network initialization and order of mini-batches. To address this issue, we re-implement the same training procedure for multiple runs. While Tensorboard lacks direct tools for visualizing averages and standard errors across multiple runs, we can still create plots to display the training curves and accuracy for different runs and compare them.\n\n\n\nWe first define our own function to visualize the average with error intervals.","metadata":{"id":"ubom96y5wcdH"}},{"cell_type":"code","source":"def plot_loss_acs(train_avg, train_err, test_avg, test_err):\n\n  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n  epochs = np.arange(np.shape(train_avg)[1])\n\n  ax1.plot(epochs,train_avg[0], label='Train')\n  ax1.fill_between(epochs,train_avg[0]+train_err[0],train_avg[0]-train_err[0], alpha=0.6)\n  ax1.plot(epochs,test_avg[0], label='Test')\n  ax1.fill_between(epochs,test_avg[0]+test_err[0],test_avg[0]-test_err[0], alpha=0.6)\n  ax1.set_xlabel('Epochs')\n  ax1.set_ylabel('Loss')\n  ax1.grid()\n  ax1.legend()\n\n  ax2.plot(epochs,train_avg[1], label='Train')\n  ax2.fill_between(epochs,train_avg[1]+train_err[1],train_avg[1]-train_err[1], alpha=0.6)\n  ax2.plot(epochs,test_avg[1], label='Test')\n  ax2.fill_between(epochs,test_avg[1]+test_err[1],test_avg[1]-test_err[1], alpha=0.6)\n  ax2.set_xlabel('Epochs')\n  ax2.set_ylabel('Accuracy')\n  ax2.grid()\n  ax2.legend()\n  plt.tight_layout()\n  plt.show()","metadata":{"id":"9pAIVGa180jC","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:09:07.798083Z","iopub.execute_input":"2024-11-27T14:09:07.799162Z","iopub.status.idle":"2024-11-27T14:09:07.811043Z","shell.execute_reply.started":"2024-11-27T14:09:07.799106Z","shell.execute_reply":"2024-11-27T14:09:07.809844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Task 1.3:** Complete the missing parts to implement multiple experimental runs of the training loop. It might take few minutes. [Hint: clean your tensorboard first].\n\n****","metadata":{"id":"vC7DAk6ww9LQ"}},{"cell_type":"code","source":"#Create a writer to write to Tensorboard\nwriter = SummaryWriter()\n\n#Set the number of epochs to for training\nepochs = 100\n\n#Set the number of runs of the experiment\nruns = 3\n\n#Initialize arrays to store meatrics\ntrain_losses = np.zeros((runs,epochs))\ntest_losses = np.zeros((runs,epochs))\ntrain_acs = np.zeros((runs,epochs))\ntest_acs = np.zeros((runs,epochs))\n\nfor run in range(runs):\n\n  # Create instance of Network\n  net = FCNet()\n\n  # Create loss function and optimizer\n  criterion = nn.CrossEntropyLoss()\n  optimizer = optim.SGD(net.parameters(), lr=5e-1)\n\n  #Shuffle test set\n  test_loader = DataLoader(test_set, batch_size=b_size, shuffle =True)\n\n  for epoch in tqdm(range(epochs)):\n\n      train_loss, train_acc = train(train_loader, net, optimizer,criterion)\n      test_loss, test_acc = test(test_loader,net,criterion)\n\n      #Create a copy of the training and test metrics \"detached\" from future gradient computations\n      train_losses[run,epoch] = train_loss.detach().numpy()\n      test_losses[run,epoch] = test_loss.detach().numpy()\n      train_acs[run,epoch] = train_acc\n      test_acs[run,epoch] = test_acc\n\n      #############################################################################\n      #                            START OF YOUR CODE                             #\n      #    Write measures to TensorBoard. Remember to distinguish different runs. #\n      #############################################################################\n      pass\n      #############################################################################\n      #                            END OF YOUR CODE                               #\n      #############################################################################\n\nprint('\\n Finished Training')\nwriter.flush()\nwriter.close()\n\n#############################################################################\n#                            START OF YOUR CODE                             #\n#        Compute averages and standard errors over runs and store them as   #\n#         train_loss_avg, train_loss_err, train_acs_avg, train_acs_err...   #\n#############################################################################\npass\n#############################################################################\n#                            END OF YOUR CODE                               #\n#############################################################################\n\ntrain_avg = np.array([train_loss_avg,train_acs_avg])\ntest_avg = np.array([test_loss_avg,test_acs_avg])\ntrain_err = np.array([train_loss_err,train_acs_err])\ntest_err = np.array([test_loss_err,test_acs_err])\n\nplot_loss_acs(train_avg, train_err, test_avg, test_err)\n\nprint(f'Final training accuracy {train_acs_avg[-1]:.0f}')\nprint(f'Final test accuracy {test_acs_avg[-1]:.0f}')","metadata":{"executionInfo":{"status":"ok","timestamp":1698086586574,"user_tz":-120,"elapsed":49839,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"f4fe37b3-96de-4655-fdaf-a42e4c8f9e0a","id":"RAAdH5JTfNR3","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:11:02.591816Z","iopub.execute_input":"2024-11-27T14:11:02.592664Z","iopub.status.idle":"2024-11-27T14:13:06.623305Z","shell.execute_reply.started":"2024-11-27T14:11:02.592624Z","shell.execute_reply":"2024-11-27T14:13:06.622188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Question 1.4:**\n\nBetween epochs 10 and 20, we see the test loss beginning to rise, while at the same time, the accuracy improves. Can you think of a reason why this apparent contradiction might occur?","metadata":{"id":"q94qAfWZkfAp"}},{"cell_type":"markdown","source":"<font color='green'> Write here your answer</font>\n****","metadata":{"id":"52njwyfZnNSr"}},{"cell_type":"markdown","source":"We will proceed with the lab, using a single experimental run to ensure a reasonable execution time.\n","metadata":{"id":"1MG8RJ_rd8ok"}},{"cell_type":"markdown","source":"# 2 Overfitting and the learning curve\n\n\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1024px-Overfitting.svg.png\" alt=\"drawing\" width=\"200\" align=\"right\" />\n\n\n\nNeural networks are large and powerful function approximators. Our model with 1 027 510 (!) trainable parameters can easily learn a decision boundary for our dataset of merely 4 000 samples. But does that mean that our model will also correctly classify data samples it has not seen before? In other words, does our model generalize to unseen data?\n\n\n\nA common practice is to divide a dataset into two different splits: a training set used to update the model parameters, and a test set which is only used to evaluate the model performance. The training curves show that although our model is able to classify all training samples with 100% accuracy, on the test set it has a rather disappointing average accuracy of only ~70%. This phenomenon is called **overfitting** and is illustrated in the image on the right: the green decision boundary is completely overfitted on the training samples, whereas the black decision boundary does not classify all training samples correctly but probably far more accurately models the true distribution of the data.\n\n\n\nA lot of factors influence the extent to which a model overfits to dataset - most importantly the size of the training set. The less samples a model can learn from, the more difficult it is to learn the true distribution of the data and the more the model will overfit. A **learning curve** shows the test accuracy or test loss of a model with respect to the number of training samples. The following code block will train the model using a varying number of samples and plot the learning curve. This can take a few minutes.","metadata":{"id":"1MClpdNxjNIO"}},{"cell_type":"code","source":"#Learning curves plot\n\ndef plot_learning_curve(train_losses, test_losses, train_acs, test_acs):\n\n  fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10, 4))\n  ax1.plot(n_samples, train_losses, marker='o', label='Train')\n  ax1.plot(n_samples, test_losses, marker='o', linestyle='--', label='Test')\n  ax1.set_ylabel('Loss')\n  ax1.set_xlabel('Training set size')\n  ax1.legend()\n  ax1.grid()\n\n  ax2.plot(n_samples, train_acs, marker='o', label='Train')\n  ax2.plot(n_samples, test_acs, marker='o', linestyle='--', label='Test')\n  ax2.set_xlabel('Training set size')\n  ax2.set_ylabel('Accuracy')\n  ax2.legend()\n  ax2.grid()\n  plt.show()\n","metadata":{"id":"fvsqOrEa7y9G","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:14:49.604196Z","iopub.execute_input":"2024-11-27T14:14:49.604642Z","iopub.status.idle":"2024-11-27T14:14:49.612333Z","shell.execute_reply.started":"2024-11-27T14:14:49.604603Z","shell.execute_reply":"2024-11-27T14:14:49.611124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_acs = []\ntest_acs = []\ntrain_losses = []\ntest_losses = []\n\nn_samples = [500, 1000, 2000, 3000, 4000]\n\n# Train model for different number of training samples\nfor n in n_samples:\n\n    # Take subset of first n samples in training set\n    train_subset = torch.utils.data.Subset(train_set, range(n))\n\n    # Create new dataloader\n    train_loader = DataLoader(train_subset,batch_size=b_size)\n\n    # Create instance of Network\n    net = FCNet()\n\n    # Create loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=5e-1)\n\n    # Set the number of epochs to for training\n    epochs = 100\n\n    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n        # Train on data\n        train_loss, train_acc = train(train_loader, net, optimizer, criterion)\n\n        # Test on data\n        test_loss, test_acc = test(test_loader, net, criterion)\n\n    # Store accuracies\n    train_acs.append(train_acc)\n    test_acs.append(test_acc)\n    train_losses.append(train_loss.detach())\n    test_losses.append(test_loss.detach())\n\nplot_learning_curve(train_losses, test_losses, train_acs, test_acs)","metadata":{"id":"SyZQqMzSScch","executionInfo":{"status":"ok","timestamp":1698075054624,"user_tz":-120,"elapsed":218089,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"1aa6d2c9-be74-4c24-8b46-1bbff285279d","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:15:43.551031Z","iopub.execute_input":"2024-11-27T14:15:43.551402Z","iopub.status.idle":"2024-11-27T14:17:40.31988Z","shell.execute_reply.started":"2024-11-27T14:15:43.551373Z","shell.execute_reply":"2024-11-27T14:17:40.318904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Question 2.1:** What does the learning curve show?","metadata":{"id":"oG9MmWIhhKDL"}},{"cell_type":"markdown","source":"<font color='green'> The figure shows that the larger the training set, the higher the test accuracy and the lower the test loss of the model become. The training accuracy and training loss are a constant 100% and ~0, respectively. For small sizes of the training set, the generalization gap is very high while the training loss remains close to $0$. This is a clear indication of overfitting.\n\n</font>\n\n****","metadata":{"id":"mxxHpzLXhWBH"}},{"cell_type":"markdown","source":"To magnify the effect of the various regularization methods we will from now on use a reduced training set size of 500 samples.","metadata":{"id":"TH4K8v0OF5y4"}},{"cell_type":"code","source":"# Take subset of first 500 samples in training set\ntrain_subset = torch.utils.data.Subset(train_set, range(500))\n\n# Create new dataloader\ntrain_loader = DataLoader(train_subset,batch_size=b_size)","metadata":{"id":"DrZXKBV9LKwW","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:18:09.452658Z","iopub.execute_input":"2024-11-27T14:18:09.453104Z","iopub.status.idle":"2024-11-27T14:18:09.45831Z","shell.execute_reply.started":"2024-11-27T14:18:09.45305Z","shell.execute_reply":"2024-11-27T14:18:09.457266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will train our model on this subset to get a baseline in Tensorboard to which we can compare our regularized models.\n\n\n\nIt might be useful to clean your Tensorboard at this point.","metadata":{"id":"_4eiOQqYLbUe"}},{"cell_type":"code","source":"# Create a writer to write to tensorboard\nwriter = SummaryWriter()\n\n# Create instance of Network\nnet = FCNet()\n\n# Create loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=5e-1)\n\n# Set the number of epochs to for training\nepochs = 100\n\nfor epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n    # Train on data\n    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n\n    # Test on data\n    test_loss, test_acc = test(test_loader,net,criterion)\n\n    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Test':test_loss}, epoch)\n    writer.add_scalars('Accuracy',{'Train': train_acc,'Test':test_acc} , epoch)\n\nprint('\\n Finished Training')\nwriter.flush()\nwriter.close()","metadata":{"id":"ujNNTMdFNbEd","executionInfo":{"status":"ok","timestamp":1698084509171,"user_tz":-120,"elapsed":16698,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"dc19f12d-68d3-49e5-d99a-05a8f329924f","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:18:44.989825Z","iopub.execute_input":"2024-11-27T14:18:44.990235Z","iopub.status.idle":"2024-11-27T14:18:53.488959Z","shell.execute_reply.started":"2024-11-27T14:18:44.990201Z","shell.execute_reply":"2024-11-27T14:18:53.487793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You will see that the training loss will continuously decrease, whereas the test loss will start to increase again at some point. Like the plateauing test accuracy, an increasing test loss is another indication of overfitting.","metadata":{"id":"WzNt1QNsqbpf"}},{"cell_type":"markdown","source":"# 3 L2 Regularization\n\n\n\nIf our neural network is able to learn such a complex decision boundary that it completely overfits on the training set, we would perhaps like to reduce its expressive power. An obvious way would be to reduce the number of parameters in our model, but that does not allow for much flexibility. Instead, we would rather have our model to prefer simpler decision boundaries without putting a hard limit on its expressivity. This is where L2 regularization comes into play:\n\n$$L = L_0 + \\frac{\\lambda}{2}\\sum_w w^2 $$\n\nHere, the total loss $L$ consists of the original task-specific loss $L_0$, e.g. cross-entropy, and the sum of the squared value of all network parameters $w$ (weights and biases), where $\\lambda$ specifies the relative weight of the two components. $\\lambda$ is a so-called hyper-parameter which needs to be specified before training, similar to the `epochs` parameter. This loss function thus encourages the network to learn small parameters, i.e. it minimizes the L2 norm of the parameters.\n\n\n\nIt might not be directly clear why this would solve our overfitting problem - an intuitive explanation is that the output of a neural network with small weights is not very susceptible to small changes in the input and therefore its recision boundary is relatively smooth. For further reading and a very nice example we'd like to refer to [this excellent online book](http://neuralnetworksanddeeplearning.com/chap3.html#why_does_regularization_help_reduce_overfitting).\n\n\n\nWe will now implement L2 normalization in our training loop. For this we will need to add the extra regularization term to our loss function. We provide you with some hints as to how to do this.","metadata":{"id":"HDM4uIkWNybv"}},{"cell_type":"markdown","source":"****\n\n**Task 3.1:** Add the L2 regularization to the loss function. The total loss becomes:``loss = loss + 0.5*wd*l2``, where l2 is the sum of the squared value of all parameters and wd the relative weight of the L2 penalty that we normally refer to as $\\lambda$. [Hint: loop through the network parameters using ``for p in net.parameters():``].  \n\n****","metadata":{"id":"hXtm32BBmi8Z"}},{"cell_type":"code","source":"def train_wd(train_loader, net, optimizer, criterion, wd):\n\n    \"\"\"\n    Trains network for one epoch in batches.\n    Uses custom L2 regularization in loss function.\n\n    Args:\n        train_loader: Data loader for training set.\n        net: Neural network model.\n        optimizer: Optimizer (e.g. SGD).\n        criterion: Loss function (e.g. cross-entropy loss).\n        wd: Weight decay (L2 penalty)\n\n    \"\"\"\n    avg_loss = 0\n    correct = 0\n    total = 0\n\n    # iterate through batches\n    for i, data in enumerate(train_loader):\n\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n\n        # Initialize L2 penalty as 0\n        l2 = 0\n\n        ########################################################################\n        #                            START OF YOUR CODE                        #\n        ########################################################################\n        # Loop through network parameters\n        pass\n        ########################################################################\n        #                          END OF YOUR CODE                            #\n        ########################################################################\n\n        # backward + optimize\n        loss.backward()\n        optimizer.step()\n        \n        # keep track of loss and accuracy\n        avg_loss += loss\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    return avg_loss/len(train_loader), 100 * correct / total","metadata":{"id":"KBCSrI8SNq6M","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:20:33.502385Z","iopub.execute_input":"2024-11-27T14:20:33.502824Z","iopub.status.idle":"2024-11-27T14:20:33.511597Z","shell.execute_reply.started":"2024-11-27T14:20:33.502788Z","shell.execute_reply":"2024-11-27T14:20:33.510486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now again train the network on the small training subset of MNIST-1D and compare the training curves with those of the baseline.\n\nDo not forget to hit refresh ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAaCAYAAACkVDyJAAABOElEQVRIDe2UsQ3CMBBFMwIjMAIbsBglJRI0dEgwCg2UwAZQQQkNpIgSox/pRx+Ls2OEIgosRXe2z/fy7bOz+yhzXX5ZlzCwfhf4mPRdsZm68rx3bPAxhjndqWK3eunrXCuFxXZGhmkBRmLA0BSifhRYXg4NBMny+aBJBp8ABFX5tYlViPpBIJVV+e0FpAngA+w3P4Z9E4hzYVNVXKhWVXKNzqtvAqkOVhf4/jsYoH4c+yaQZxdSZ8E+Asa2hn+cak2FnQPbbGmqOsSbCtsWjUJ5pqFCM4Ep1wJQvYv+U6c/ZQIRRJV4QULVijm+MiF1yBkEIoBniSIqdssG/Bj3XL4Y1mMssPK4Nu8fVUaBqpSJ39mYsiQggnEuSKqKq+upHgudGUG0rRQy+Bv2D4xWXeo2/7f061v6BP5uhjxw+N1/AAAAAElFTkSuQmCC) in the top right corner of Tensorboard to display the results and compare with the baseline.\n","metadata":{"id":"QPkjD_14oEWM"}},{"cell_type":"code","source":"# Create a writer to write to tensorboard\nwriter = SummaryWriter()\n\n# Create instance of Network\nnet = FCNet()\n\n# Create loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=5e-1)\n\n# Set the number of epochs to for training\nepochs = 100\n\n# Set weighing factor for L2 regularization\nwd = 3e-3\n\nfor epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n\n    # Train on data\n    train_loss, train_acc = train_wd(train_loader,net,optimizer,criterion,wd)\n\n    # Test on data\n    test_loss, test_acc = test(test_loader,net,criterion)\n\n    writer.add_scalars('Loss', {'Train': train_loss, 'Test':test_loss}, epoch)\n    writer.add_scalars('Accuracy',{'Train': train_acc,'Test':test_acc} , epoch)\n\n\nprint('Finished Training')\nwriter.flush()\nwriter.close()","metadata":{"id":"5rK2foVjUVZQ","executionInfo":{"status":"ok","timestamp":1698084578459,"user_tz":-120,"elapsed":18675,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"f7737028-96e0-4a85-d75c-d46bc2da879e","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:21:10.571774Z","iopub.execute_input":"2024-11-27T14:21:10.572561Z","iopub.status.idle":"2024-11-27T14:21:21.07008Z","shell.execute_reply.started":"2024-11-27T14:21:10.572524Z","shell.execute_reply":"2024-11-27T14:21:21.06903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Question 3.2:** What is the effect of the regularization term?\n","metadata":{"id":"-zlpVOpcoH6Q"}},{"cell_type":"markdown","source":"<font color='green'> Write here your answer\n</font>\n\n****","metadata":{"id":"4T_E6mA-oVez"}},{"cell_type":"markdown","source":"In PyTorch, *weight decay*, which is simply another name for L2 regularization, is very conveniently built into the SGD optimizer.","metadata":{"id":"V14N5iVcrAfj"}},{"cell_type":"markdown","source":"****\n\n**Task 3.3:** Have a look at the documentation [here](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) and initialize the optimizer with the proper settings to train the network using the PyTorch built-in weight decay functionality. Set the weighting factor $\\lambda$ for L2 regularization to 3e-3 as before.\n\n****","metadata":{"id":"RrHUisWlqQCO"}},{"cell_type":"code","source":"#Create a writer to write to tensorboard\nwriter = SummaryWriter()\n\n# Create instance of Network\nnet = FCNet()\n\n# Create loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n########################################################################\n#                            START OF YOUR CODE                        #\n########################################################################\noptimizer = None\n########################################################################\n#                          END OF YOUR CODE                            #\n########################################################################\n\n#Set the number of epochs to for training\nepochs = 100\n\nfor epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n\n    # Train on data\n    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n\n    # Test on data\n    test_loss, test_acc = test(test_loader,net,criterion)\n\n    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Test':test_loss}, epoch)\n    writer.add_scalars('Accuracy',{'Train': train_acc,'Test':test_acc} , epoch)\n\nprint('Finished Training')\nwriter.flush()\nwriter.close()","metadata":{"id":"BBApWFhUxa6p","executionInfo":{"status":"ok","timestamp":1698084608888,"user_tz":-120,"elapsed":17421,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"451fcd45-b656-4248-80a9-0f109d89cf7b","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:24:58.275405Z","iopub.execute_input":"2024-11-27T14:24:58.275846Z","iopub.status.idle":"2024-11-27T14:25:07.326087Z","shell.execute_reply.started":"2024-11-27T14:24:58.275806Z","shell.execute_reply":"2024-11-27T14:25:07.325082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Have a look again at the loss curves in Tensorboard - you should see the same effect as with our own L2 regularization above.","metadata":{"id":"xsVvTz9lkW3G"}},{"cell_type":"markdown","source":"****\n\n**Question 3.4:** Can you think of a reason why the training losses with PyTorch and our own implementation do not look similar? What do you think PyTorch returns as loss and what did we plot in our own implementation? \n","metadata":{"id":"G_FZyurFw8de"}},{"cell_type":"markdown","source":"<font color ='green'> Write your answer here </font>\n\n****","metadata":{"id":"3rjQO9uFxDkH"}},{"cell_type":"markdown","source":"# 4 Early stopping\n\n\n\nInstead of training a neural network for a fixed amount of epochs we can choose to halt training when the network starts to overfit on the training data. This is called **early stopping**.\n\n\n\nFor early stopping we continuously monitor the performance of the model on a separate *validation set* and stop when a certain metric, e.g. the accuracy, no longer improves. In practice this metric will not improve monotonously but will go up and down due to the stochastic nature of deep neural network training. Therefore often a certain *patience* is used, i.e. training is halted after the metric has not improved for a certain number of epochs. The patience is a hyper-parameter to be specified before training.\n\n\n\nYou might wonder why we need a separate validation test and do not simply use the test set to monitor the network performance. In general we always use a separate validation set for monitoring network performance and tuning hyper-parameters because we do not want to overfit these settings to the specifities of our test set. This way the performance on the test set remains a true measure of how well our model generalizes.\n\n\n\nLet us start by making the validation set. Since we are only using the first 500 samples of the training set, we can use samples 500-1500 for validation.","metadata":{"id":"PZT7cB_CVYL6"}},{"cell_type":"code","source":"# Make validation set - take samples 500-1500 from training set for convenience\nval_set = torch.utils.data.Subset(train_set, range(500,1500))\n\n# Create new dataloader\nval_loader = DataLoader(val_set,batch_size=b_size)","metadata":{"id":"SKhkuoakurQi","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:27:44.1633Z","iopub.execute_input":"2024-11-27T14:27:44.163826Z","iopub.status.idle":"2024-11-27T14:27:44.172412Z","shell.execute_reply.started":"2024-11-27T14:27:44.163778Z","shell.execute_reply":"2024-11-27T14:27:44.1703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You will now implement early stopping in the training loop. In short, the pseudo code for early stopping can be written as:\n\n```python\n\nfor i in range(epochs):\n\n    train(model)\n\n    metric = validate(model)\n\n    if metric > metric_best:\n\n        metric_best = metric\n\n        reset patience_counter\n\n    else:\n\n        increment patience_counter\n\n        if patience_counter == patience:\n\n            halt training\n\n```\n\n\n","metadata":{"id":"Y1MiKvif8_VE"}},{"cell_type":"markdown","source":"****\n\n**Task 4.1:** Set the value for the `patience`. Then implement early stopping. [Hint: use 'break' to break out of the training loop.]\n\n****","metadata":{"id":"-Z_HCCNfuB7O"}},{"cell_type":"code","source":"# Create a writer to write to tensorboard\nwriter = SummaryWriter()\n\n# Create instance of Network\nnet = FCNet()\n\n# Create loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=5e-1, weight_decay=3e-3)\n\n# Set the number of epochs to for training\nepochs = 100\n\n# Patience - how many epochs to keep training after accuracy has not improved\npatience = 0\n\n# Initialize early stopping variables\nval_acc_best = 0\npatience_cnt = 0\n\nfor epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n\n    # Train on data\n    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n\n    # Test on validation set\n    val_loss, val_acc = test(val_loader,net,criterion)\n\n    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Val': val_loss}, epoch)\n    writer.add_scalars('Accuracy', {'Train': train_acc, 'Val': val_acc}, epoch)\n\n    ########################################################################\n    #                            START OF YOUR CODE                        #\n    ########################################################################\n    pass\n    ########################################################################\n    #                          END OF YOUR CODE                            #\n    ########################################################################\n\n# Test on test set\ntest_loss, test_acc = test(test_loader,net,criterion)\nprint('\\n Finished Training')\nprint('Validation accuracy:\\t{:.2f}'.format(val_acc))\nprint('Test accuracy:\\t\\t{:.2f}'.format(test_acc))\n\nwriter.flush()\nwriter.close()","metadata":{"id":"2Z-7Ph5XuAnW","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:28:59.603931Z","iopub.execute_input":"2024-11-27T14:28:59.604381Z","iopub.status.idle":"2024-11-27T14:29:08.638185Z","shell.execute_reply.started":"2024-11-27T14:28:59.604344Z","shell.execute_reply":"2024-11-27T14:29:08.637097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Question 4.2:** Do you think early stopping really improves the model performance in this case?\n\n****","metadata":{"id":"y4t6bUXUusm5"}},{"cell_type":"markdown","source":"# 5 Dropout\n\n\n\nDropout is a regularization method that modifies the network architecture itself rather than the way the network is optimized. A Dropout layer deactivates a random fraction $p$ of the neurons in a layer during a training iterations by setting them to zero. After performing the forward and backward pass for the current iteration, a different set of neurons is deactivated.\n\n\n\nThere are different explanations for the regularizing effect of Dropout, most popularly:\n\n* When deactivating a different set of neurons in each training iteration, it is like we are training many different models. The final model, where all neurons are active, will then act as a model ensamble.\n\n* A related explanation is that a neuron becomes less reliant on the presence of other specific neurons and is therefore forced to learn a robust feature that works with many different sets of neurons. This can be seen as noise injection, which makes the neurons more robust to small peturbations in the input and in that sense makes it somewhat similar to L2 regularization.\n\n\n\nDropout is very easy to use in PyTorch. Have a look at the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and implement some Dropout layers in our model.","metadata":{"id":"W9nhiFVEHbVY"}},{"cell_type":"markdown","source":"****\n\n**Task 5.1:** Set the parameter `p` and add some Droupout layers. Then modify the forward pass to include the Droupout layers.\n\n****","metadata":{"id":"1O0wk8Y7-Eiv"}},{"cell_type":"code","source":"class FCNet_do(nn.Module):\n\n    \"\"\"\n    Simple fully connected neural network with residual connections and dropout\n    layers in PyTorch. Layers are defined in __init__ and forward pass\n    implemented in forward.\n\n    \"\"\"\n\n    def __init__(self):\n        \n        super(FCNet_do, self).__init__()\n        \n        self.fc1 = nn.Linear(40, 500)\n        self.fc2 = nn.Linear(500, 500)\n        self.fc3 = nn.Linear(500, 500)\n        self.fc4 = nn.Linear(500, 500)\n        self.fc5 = nn.Linear(500, 500)\n        self.fc6 = nn.Linear(500, 10)\n\n        ########################################################################\n        #                            START OF YOUR CODE                        #\n        ########################################################################\n        pass\n        ########################################################################\n        #                          END OF YOUR CODE                            #\n        ########################################################################\n\n    def forward(self, x):\n\n        ########################################################################\n        #                            START OF YOUR CODE                        #\n        ########################################################################\n        pass\n        ########################################################################\n        #                          END OF YOUR CODE                            #\n        ########################################################################\n\n        return self.fc6(h)","metadata":{"id":"q0ontWUwHe5V","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:31:35.794301Z","iopub.execute_input":"2024-11-27T14:31:35.794704Z","iopub.status.idle":"2024-11-27T14:31:35.804199Z","shell.execute_reply.started":"2024-11-27T14:31:35.794671Z","shell.execute_reply":"2024-11-27T14:31:35.803076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a writer to write to tensorboard\nwriter = SummaryWriter()\n\n# Create instance of Network\nnet = FCNet_do()\n\n# Create loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=5e-1, weight_decay=3e-3)\n\n#Set the number of epochs to for training\nepochs = 100\n\nfor epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n\n    # Train on data\n    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n\n    # Test on data\n    test_loss, test_acc = test(test_loader,net,criterion)\n\n    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Test': test_loss}, epoch)\n    writer.add_scalars('Accuracy', {'Train': train_acc, 'Test': test_acc}, epoch)\n\nprint('Finished Training')\nwriter.flush()\nwriter.close()","metadata":{"id":"aAPlLrHqHe-w","executionInfo":{"status":"ok","timestamp":1698088841754,"user_tz":-120,"elapsed":16646,"user":{"displayName":"Elena Congeduti","userId":"06963343184623262712"}},"outputId":"9629adb5-0c6e-48ae-9c04-9d057b1817ac","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:32:05.542526Z","iopub.execute_input":"2024-11-27T14:32:05.543681Z","iopub.status.idle":"2024-11-27T14:32:14.773971Z","shell.execute_reply.started":"2024-11-27T14:32:05.543636Z","shell.execute_reply":"2024-11-27T14:32:14.77297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****\n\n**Question 5.2:**  Do you think Dropout really improves the model performance in this case?\n\n****","metadata":{"id":"a1uafsyH9tyZ"}},{"cell_type":"markdown","source":"**That's all for this lab, see you in the next one!**","metadata":{"id":"V36bVnxeBBwr"}},{"cell_type":"markdown","source":"**Feedback Form:** please fill in the following form to provide feedback https://forms.office.com/e/fbeyeBH4BS","metadata":{}}]}